name: Add links to archive.org
on:
  #push:
  #  branches: [ main ]
  workflow_dispatch:
jobs:
  gather-links:
    runs-on: ubuntu-latest
    steps:
      - name: Gather all links from the page using PowerShell
        shell: pwsh
        run: |
          function Get-LinksFromSiteMap($url) {
            # get sitemap
                try {
                    $sitemap = Invoke-WebRequest -Uri $url -UseBasicParsing
                    # convert from XML to object
                    $xml = [xml]$sitemap.Content
                    return($xml.urlset.url.loc)
                } catch {
                    Write-Error "Failed to retrieve sitemap. Error: $($_.Exception.Message)"
                    exit 1
                }
            }

            $filterOutPrefixes = @("https://www.facebook.com/sharer/*","https://www.linkedin.com/sharing/share-offsite/*","https://twitter.com/intent/tweet/*")

            $siteBase = "https://diecknet.de"
            

            # (language specific sitemaps) 
            # probably not the most efficient way when adding two arrays, but it works
            $allLinks = (Get-LinksFromSiteMap "$sitebase/de/sitemap.xml")+(Get-LinksFromSiteMap "$sitebase/en/sitemap.xml")
            


            $linkCollection = foreach($link in $allLinks) {
                try {
                    $page = Invoke-WebRequest -Uri $link -UseBasicParsing
                    # :linkCheck is a break label for continue later below
                    :linkCheck foreach($foundLink in ($page.Links.href)) {
                        if($foundLink -like "#*") { # if the link is an anchor link
                            continue
                        }
                        if($foundLink -like "/*") { # if the link is a relative link
                            continue
                        }
                        if($foundLink -like "$($siteBase)/*") { # if the link is internal
                            continue
                        }
                        if([string]::IsNullOrWhiteSpace($foundLink)) { # if the link is empty
                            continue
                        }
                        foreach($filter in $filterOutPrefixes) { # filter out specific url prefixes
                            if($foundLink -like $filter) {
                                continue linkCheck # continue with next link, not the current forEach loop here
                            }
                        }
                        $foundLink #output to foreach loop
                    }
                } catch {
                    Write-Error "Failed to retrieve links from page ($($link)). Error: $($_.Exception.Message)"
                }
            }
            # output unique external links plus internal to csv
            $linkCollection+$allLinks | Select-Object -Unique | Out-File links.csv -Encoding utf8

      - name: Upload linklist artifact
        uses: actions/upload-artifact@v3
        with:
          name: linklist
          path: ./links.csv
          if-no-files-found: error
  add-links:
    needs: gather-links
    runs-on: ubuntu-latest
    container:
      image: secsi/waybackpy:3.0.6
      options: --cpus 1
    steps:
      - name: Download linklist artifact
        uses: actions/download-artifact@v3
        with:
          name: linklist
      - name: Go through linklist and add every link to archive.org using waybackpy
        # waiting for a few seconds because the API is rate-limited
        run: |
          while read LINK; do
            waybackpy --url "$LINK" --user_agent "diecknet-blog-archiver" --save
            sleep 16
          done < "links.csv"
